{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcJbNeNBW-O0",
        "outputId": "ee6a1763-37d9-43c4-d999-93b9d5bf5ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka2VyPW6W_vh"
      },
      "outputs": [],
      "source": [
        "# dataset.zip 압축 풀기\n",
        "!unzip --qq /content/drive/MyDrive/spectrograms.zip -d spectrograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCzfu3F7vY4L"
      },
      "outputs": [],
      "source": [
        "# dataset.zip 압축 풀기\n",
        "!unzip --qq /content/drive/MyDrive/ptb-xl.zip -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyBgm8_1Yq97",
        "outputId": "6db9b255-9d94-413e-9495-21ec1b827793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->wfdb) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.8.1->wfdb) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.16.0)\n",
            "Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/160.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wfdb\n",
            "Successfully installed wfdb-4.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install wfdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s0dx4W4NWCI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lU9wMX1iY3r"
      },
      "outputs": [],
      "source": [
        "# 경고 메시지 숨기기\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od5Hk8R4_zrI"
      },
      "outputs": [],
      "source": [
        "# Dataset Class\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, root_folder, img_size=(128, 128)):\n",
        "        \"\"\"\n",
        "        Initialize the dataset by loading spectrogram images from folders.\n",
        "        Args:\n",
        "            root_folder (str): Root directory containing spectrograms organized by class.\n",
        "            img_size (tuple): Size to resize images to (height, width).\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.classes = []\n",
        "\n",
        "        # Load images and labels\n",
        "        for class_idx, class_name in enumerate(os.listdir(root_folder)):\n",
        "            class_path = os.path.join(root_folder, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                self.classes.append(class_name)\n",
        "                for filename in os.listdir(class_path):\n",
        "                    if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "                        file_path = os.path.join(class_path, filename)\n",
        "                        self.data.append(file_path)\n",
        "                        self.labels.append(class_idx)\n",
        "\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        img_path = self.data[idx]\n",
        "        image = Image.open(img_path).convert(\"L\")  # Grayscale\n",
        "        image = image.resize(self.img_size)  # Resize to target size\n",
        "        image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
        "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        # Load label\n",
        "        label = self.labels[idx]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mwein6g_19f"
      },
      "outputs": [],
      "source": [
        "# CNN2D Model\n",
        "class CNN2D(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN2D, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(10, 10))\n",
        "        self.conv2 = nn.Conv2d(in_channels=100, out_channels=250, kernel_size=(10, 10))\n",
        "        self.conv3 = nn.Conv2d(in_channels=250, out_channels=500, kernel_size=(10, 10))\n",
        "        self.conv4 = nn.Conv2d(in_channels=500, out_channels=1000, kernel_size=(10, 10))\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(100)\n",
        "        self.bn2 = nn.BatchNorm2d(250)\n",
        "        self.bn3 = nn.BatchNorm2d(500)\n",
        "        self.bn4 = nn.BatchNorm2d(1000)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(1000, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o7vPj4O__FP"
      },
      "outputs": [],
      "source": [
        "def main(spectrograms_dir):\n",
        "    # 스펙트로그램 이미지와 슈퍼클래스 레이블 로드\n",
        "    images, labels = load_spectrograms_with_superclass(spectrograms_dir)\n",
        "\n",
        "    # 데이터셋 분할\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        images, labels, test_size=0.1, random_state=42, stratify=labels\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=0.2222, random_state=42, stratify=y_train_val\n",
        "    )\n",
        "\n",
        "    print(f\"Total images: {len(images)}\")\n",
        "    print(f\"Train set: {len(X_train)}, Validation set: {len(X_val)}, Test set: {len(X_test)}\")\n",
        "    print(f\"Class distribution:\")\n",
        "    print(f\"0 (Normal): {sum(1 for label in labels if label == 0)}\")\n",
        "    print(f\"1 (Hyperdisease): {sum(1 for label in labels if label == 1)}\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),  # 흑백 변환\n",
        "        transforms.Resize((128, 128)),                # 고정된 크기로 변환\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    train_dataset = SpectrogramDataset(X_train, y_train, transform)\n",
        "    val_dataset = SpectrogramDataset(X_val, y_val, transform)\n",
        "    test_dataset = SpectrogramDataset(X_test, y_test, transform)\n",
        "\n",
        "    # 데이터 로더 생성\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "    # 모델 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # ResNet 구성: [2, 2, 2, 2]는 ResNet18의 레이어 구성\n",
        "    model = CNN2D_Binary()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 손실 함수와 옵티마이저\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # 모델 훈련\n",
        "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
        "\n",
        "    # 테스트 세트 평가\n",
        "    trained_model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "       # 혼동 행렬을 위한 변수\n",
        "    confusion_matrix = torch.zeros(2, 2, dtype=torch.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = trained_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()  # 확률 > 0.5를 기준으로 클래스 결정\n",
        "\n",
        "            # 혼동 행렬 업데이트\n",
        "            for t, p in zip(labels, predicted):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * test_correct / test_total\n",
        "    print(f'\\nTest Accuracy: {test_accuracy:.2f}%')\n",
        "    print('\\n혼동 행렬:')\n",
        "    print(confusion_matrix)\n",
        "\n",
        "    # 정밀도, 재현율, F1 점수 계산\n",
        "    tn, fp, fn, tp = confusion_matrix.numpy().ravel()\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f'\\n정밀도: {precision:.4f}')\n",
        "    print(f'재현율: {recall:.4f}')\n",
        "    print(f'F1 점수: {f1_score:.4f}')\n",
        "\n",
        "    # 혼동 행렬 시각화\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xticks([0, 1], ['Normal', 'Hyperdisease'])\n",
        "    plt.yticks([0, 1], ['Normal', 'Hyperdisease'])\n",
        "\n",
        "    # 혼동 행렬 값 추가\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, str(confusion_matrix[i, j].item()),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if confusion_matrix[i, j] > confusion_matrix.max()/2 else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "vB64etFEAEFY",
        "outputId": "aab028a7-bdd3-4f4c-edd3-050922d00f1f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6c42a5dc4ebd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mspectrograms_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/spectrograms'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrograms_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-ad21987d5c9a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(spectrograms_dir)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 데이터셋 분할\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     X_train_val, X_test, y_train_val, y_test = train_test_split(\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2786\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2416\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    spectrograms_dir = '/content/spectrograms'\n",
        "    main(spectrograms_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}